%!TEX program = xelatex

\documentclass[compress]{beamer}
%--------------------------------------------------------------------------
% Common packages
%--------------------------------------------------------------------------

\definecolor{links}{HTML}{663000}
\hypersetup{colorlinks,linkcolor=,urlcolor=links}

\usepackage[english]{babel}
\usepackage{pgfpages} % required for notes on second screen
\usepackage{graphicx}

\usepackage{multicol}

\usepackage{tabularx,ragged2e}
\usepackage{booktabs}

\setlength{\emergencystretch}{3em}  % prevent overfull lines

\usetheme{hri}

% Display the navigation bullet even without subsections
\usepackage{remreset}% tiny package containing just the \@removefromreset command
\makeatletter
\@removefromreset{subsection}{section}
\makeatother
\setcounter{subsection}{1}


\newcommand{\source}[2]{{\tiny\it Source: \href{#1}{#2}}}

\usepackage{tikz}
\usetikzlibrary{mindmap,backgrounds,positioning,calc,patterns}

\graphicspath{{figs/}}

\title{Social Signal Processing}
\subtitle{AINT512}
\date{}
\author{Séverin Lemaignan}
\institute{Centre for Neural Systems and Robotics\\{\bf Plymouth University}}

\begin{document}

\licenseframe{github.com/severin-lemaignan/lectures-hri}

\maketitle

\section[Social signals?]{What are Social Signals?}

\videoframe[0.56]{figs/tears-of-steel-extract.mp4}
\imageframe{social-signal}


{
    \paper{Burgoon, Magnenat-Thalmann, Pantic, Vinciarelli, {\bf Social Signal Processing}, 2017}

\begin{frame}{What are social signals?}


    \begin{itemize}
        \item<1-> Social signals are \emph{observable} behaviours that people
            display during social interactions
        \item<2-> Social signals from individual A \emph{produces changes} in others
            (like creating a belief about A, generating an appropriate social
            response, perform an actions)
        \item<3-> the changes are not random, they follow \emph{principles and
            laws} (in particular, \emph{social norms})

    \end{itemize}

\end{frame}
}

\begin{frame}{Why?}
\begin{itemize}

\item The ability to recognize human social signals and social behaviours
  like turn taking, politeness, and disagreement.
\item Essential when building social robots, human-robot interaction,
  interactive systems, \ldots{}
\end{itemize}

    \begin{exampleblock}{3 main problems}

    \begin{itemize}
        \item \emph{Modeling}: identification of the principles and laws
        \item \emph{Analysis}: automatic detection and interpretation
        \item \emph{Synthesis}: automatic generation of artificial social
            signals
    \end{itemize}

    \end{exampleblock}
\end{frame}

\begin{frame}{Is it hard?}

    On the following video, pay particular attention to turn taking
\end{frame}

\videoframe[0.56]{figs/social-signal.mkv}

\begin{frame}{Is it hard?}

As with most human activities that seem easy to us, social signal
processing is tremendously hard.

Often broken down in smaller, sometimes more manageable, tasks:

\begin{itemize}

\item People detection
\item Face detection
\item Face recognition
\item Gesture recognition
\item Gaze detection
\item Facial expression reading (wink, blink, talking, \ldots{})
\item Detection of social signals from verbal communication
\item Emotion recognition (from faces, movement, speech, \ldots{})
\item \ldots{}
\end{itemize}

\end{frame}

\begin{frame}{In this lecture}

\begin{itemize}

\item
  A historical case study: Kismet.
\item
  Recognising emotions from speech.
\item
  Speech recognition in the context of HRI.
\item
  Classification (k-nearest neighbours and Support Vector Machines)
\item
  Classification for social signal processing
\end{itemize}

\end{frame}


\begin{frame}{Next 2 weeks}

    More technical:

    \begin{itemize}
        \item How to extract facial features
        \item How to estimate gaze
        \item How to measure attention
        \item How to recognise a face
        \item How to model emotions
    \end{itemize}

\end{frame}

\begin{frame}[plain]
\centering
        \resizebox{!}{0.9\paperheight}{%
            \begin{tikzpicture}[
                    >=latex,
                every edge/.style={<-, draw, very thick}]
        

            \path[small mindmap, 
                level 1 concept/.append style={sibling angle=90}, 
                level 2 concept/.append style={sibling angle=60}, 
            concept color=white,text=hriWarmGreyDark]
            node[concept, visible on=<1-3>] {\bf Social\\Signals...}
            [clockwise from=0]
            child[concept color=hriSec2] { node[concept] (percept){...on the face}
                [clockwise from=30]
                child[concept color=hriSec3Dark,text=white] { node[concept]
                (emotions) {Emotions} }
                child[concept color=hriSec2Dark,text=white] { node[concept] (attention) {Gaze} };
            }
            child[concept color=hriSec2Comp,text=white,visible on=<2->] { node[concept] (knowledge) {...from the body}
                [counterclockwise from=-150]
                child[concept color=hriSec1CompDark,text=white] { node[concept] (soc-rules) {Proxemics} }
                child[concept color=hriSec3Comp,text=black] { node[concept] (soc-ctxt) {Body posture} }
                child[concept color=hriSec2Dark,text=white] { node[concept] (memory) {Gestures} };
            }
            child[concept color=hriSec3Comp,text=black,visible on=<3->] { node[concept] (comm) {...in the voice} 
                [counterclockwise from=90]
                child[concept color=hriSec1CompDark,text=white] { node[concept] (dialog) {Prosody} }
                child[concept color=hriSec3,text=white] { node[concept] (dialog) {Verbal communication} }
                child[concept color=hriSec1Dark,text=white] { node[concept] (non-verbal) {Non-verbal} };
            };


        \end{tikzpicture}
    }
\end{frame}

\section[Kismet]{Case study: Kismet's vision system}


\begin{frame}{Case study: Kismet's vision system}

    \only<1>{
        Remember Kismet?

    \begin{center}
    \video{0.7\paperwidth}{figs/kismet.mp4}
    \end{center}



    Built by Cynthia Breazeal and a team of postgrad students at the
    Massachusetts Institute of Technology (MIT) in 1997.
}
    \only<2>{

        Primary goal: implement an \textbf{attention system}: directing gaze
    towards salient features.

    \begin{itemize}
        \item This should be real-time.
        \item Should resemble the attention system of human infants.
    \end{itemize}

    \begin{columns}
        \begin{column}{0.4\linewidth}
    Attention to several cues:

    \begin{itemize}
        \item Skin tone
        \item Saturated colours
        \item Motion
    \end{itemize}
            
        \end{column}
        \begin{column}{0.6\linewidth}
    \begin{center}
        \includegraphics[width=\columnwidth]{colormap}
    \end{center}
        \end{column}
    \end{columns}


    Note that detecting skin tone is easier than detecting faces, and often
    has the same result as face detection
}
\end{frame}

\imageframe{attention}

\begin{frame}{Skin tone feature detector}

    Incoming stream is 8-bit colour images, 128 by 128 pixels\bubblemark{128x128}.


    \bubble<1>[70][0.7][5cm]{128x128}{
  128 x 128 resolution limit caused by hardware throughput and
  computation available at the time, current hardware will be able to
    handle megapixel resolution in real-time.}

    A pixel is \emph{not} skin-toned if:

\begin{itemize}
\item $R < 1.1 G$ and
\item $R < 0.9 B$ and
\item $R > 2.0 \times max(G,B)$ and
\item $R < 20$ and
\item $R > 250$
\end{itemize}

This simple procedure works for most skin colours and under most
lighting conditions.

  It is claimed this even works with non-Caucasian skin, as it picks up
  a colour component in the blood.


\end{frame}

\begin{frame}{Skin tone feature detector}

    \begin{center}
        \includegraphics[width=\linewidth]{skin-tone}
    \end{center}

    \begin{columns}
        \begin{column}{0.5\linewidth}
            Cut through RGB 3-dimensional space, picking out only skin tone RGB
            values


        \end{column}
        \begin{column}{0.5\linewidth}
            Result of skin tone detector in action.
        \end{column}
    \end{columns}

\end{frame}

\begin{frame}[fragile]{Saturation detector}

Convert RGB to HSV values

    \begin{columns}
        \begin{column}{0.5\linewidth}
\begin{itemize}

    \item From \texttt{<red, green, blue>} to \texttt{<hue, saturation, value>} representation.
\item \textbf{hue} is the ``colour'', \textbf{saturation} is how deep the
  colour is and \textbf{value} is how bright the pixel is.
\item Usually an excellent first step in computer vision when processing
  colour images, as it removes the effect of uneven lighting.
\end{itemize}
            
        \end{column}
        \begin{column}{0.5\linewidth}

    \begin{center}
        \includegraphics[width=\columnwidth]{hsv}
    \end{center}

        \end{column}
    \end{columns}


\end{frame}

\begin{frame}[fragile]{RGB to HSV in Python}

    \begin{columns}
        \begin{column}{0.5\linewidth}
\begin{pythoncode}
def rgb2hsv(r, g, b):
    r, g, b = r/255.0, g/255.0, b/255.0
    mx = max(r, g, b)
    mn = min(r, g, b)
    df = mx-mn
    if mx == mn:
        h = 0
    elif mx == r:
        h = (60 * ((g-b)/df) + 360) % 360
    elif mx == g:
        h = (60 * ((b-r)/df) + 120) % 360
    elif mx == b:
        h = (60 * ((r-g)/df) + 240) % 360
    if mx == 0:
        s = 0
    else:
        s = df/mx
    v = mx
    return h, s, v
\end{pythoncode}
\source{http://code.activestate.com/recipes/576919-python-rgb-and-hsv-conversion/}{Python recipes}
            
        \end{column}
        \begin{column}{0.5\linewidth}
            \vspace{4cm}

or better:

\begin{pythoncode}
import colorsys
colorsys.rgb_to_hsv(0.2, 0.4, 0.4)
--> (0.5, 0.5, 0.4)
colorsys.hsv_to_rgb(0.5, 0.5, 0.4)
--> (0.2, 0.4, 0.4)
\end{pythoncode}

        \end{column}
    \end{columns}


\end{frame}

\begin{frame}[fragile]{Saturation detector}

\begin{itemize}

\item
  Convert image from RGB to HSV, and mark pixels (by setting them to
  WHITE) that have a saturation over a certain threshold.
\item
  Focus on \textbf{Centre of Gravity} of all saturation pixels.
\end{itemize}

\begin{matlabcode}
void centerOfGravity (int[][] image, int imageWidth,
        int imageHeight)
{
    int SumX = 0;
    int SumY = 0;
    int num = 0;
    for (int i=0; i&lt;imageWidth; i++)
    {
        for (int j=0; j&lt;imageHeight; j++)
        {
            if (image[i][j] == WHITE)
            {
                SumX = SumX + i;
                SumY = SumY + j;
                num++;
            }
        }
    }
    SumX = SumX / num;
    SumY = SumY / num;
    // The coordinate (SumX,SumY) is the center of gravity
}
\end{matlabcode}

\end{frame}

\begin{frame}[fragile]{Motion detector}

\begin{itemize}

\item
  Motion is a strong social signal, essential for survival (both in
  detecting prey and predator). Robots are expected to be sensitive to
  motion as well.
\end{itemize}

\begin{matlabcode}
int[][] motionDetection (int[][] image,
        int[][] imagePrevious, int imageWidth, int
        imageHeight)

{
    int[][] imageMotion;
    int num = 0;
    imageMotion = (int\\)malloc( imageWidth \ imageHeight)

    for (int i=0; i&lt;imageWidth; i++)
    {
        for (int j=0; j&lt;imageHeight; j++)
        {
            imageMotion[i][j] = abs(image[i][j] - imagePrevious[i][j]);
        }
    }
}
\end{matlabcode}

\end{frame}

\begin{frame}{Optical flow}

    Today, motion detection is performed by computing the \emph{optical flow}
    of a video stream.

    \video{0.8\paperwidth}{figs/optical_flow.mp4}

    The colours indicate the direction in which each pixel moves.


\end{frame}


\begin{frame}{Post-attentive processing}

After attention has been fixed, run some additional algorithms

\begin{itemize}

\item Eye detection: using template matching.
\item Proximity estimation: using stereo vision.
\item Loom detection: if the size of the motion blob increases rapidly.
\item Threat detection: rapid motion or fast looming.
\end{itemize}

The results of these visual processes feed into the behaviour manager of
the robot.

\begin{itemize}

\item
  Robot focuses on visual regions that draw its attention
\end{itemize}

\end{frame}

\begin{frame}{Selection of attention}

Kismet does not attend to all three features for computational
reasons\bubblemark{processingpower}, but selects one according to its
current behaviour.

For example, when the {\tt seek\_people} behaviour is active, it uses skin tone
to direct its attention.

\bubble<1>[110][1.7][5cm]{processingpower}{ This was back in early 2000s.
Current hardware can handle this computational load without problem.
However, it is good practice to not spend processor cycles on signals you
do not require.  }

    \onslide<2>{
Implementation: an \textbf{attention activation map}

\begin{itemize}
    \item A 128 by 128 map of activation for the feature detectors is
    constructed.
    \item A threshold is applied to get rid of noise.
    \item Connected regions are found using the 4-connect algorithm.
    \item Regions of less than 30 pixels are thrown away.
    \item Centroid is computed.
    \item Attention is drawn to largest region.
\end{itemize}

The attention map is independent of the feature (color, motion, \ldots{})
}

\end{frame}

\begin{frame}{The auditory system}

Kismet does not understand human language!

It only pays attention to emotional content of speech. This is done
though monitoring \textbf{prosody}.

Advantages:

\begin{itemize}

\item rather easy and computationally inexpensive
\item the robot is not committed to one single language
\item the robot is speaker independent
\end{itemize}

\end{frame}

\begin{frame}{Emotion from prosody}

„Sie haben es gerade hochgetragen und jetzt gehen sie wieder runter``
(They just carried it upstairs and now they are going down again).

    \begin{center}
    \video[1]{1cm}{figs/prosody/media1.mp4}\hspace{0.3em}
    \video[1]{1cm}{figs/prosody/media2.mp4}\hspace{0.3em}
    \video[1]{1cm}{figs/prosody/media3.mp4}\hspace{0.3em}
    \video[1]{1cm}{figs/prosody/media4.mp4}\hspace{0.3em}
    \video[1]{1cm}{figs/prosody/media5.mp4}
    \end{center}

    \source{http://emodb.bilderbar.info/start.html}{Berlin Database of
  Emotional Speech}


Which emotion do you recognise?


    \textbf{Anger} -- \textbf{Boredom} -- \textbf{Disgust} -- \textbf{Anxiety/Fear} -- \textbf{Happiness} --
    \textbf{Sadness} --  \textbf{Neutral}

Humans recognise emotions from prosody with approx. 80\% accuracy
(\textit{caveat: this is for speech spoken by actors in a recording studio})

\end{frame}

\begin{frame}{Speech processing system}

Compute features from the samples

\begin{itemize}

\item
  Pitch mean
\item
  Pitch variance
\item
  Maximum pitch.
\item
  Minimum pitch.
\item
  Pitch range.
\item
  \ldots{}
\end{itemize}

    \begin{center}
        \includegraphics[width=0.8\linewidth]{prosody}
    \end{center}

\end{frame}

\begin{frame}{Audio processing stream in Kismet}

    Many different samples were recorded and classified by a human subject.
    Then an algorithm was trained (Expectation Maximisation) to classify the
    samples into five distinct classes.

    \begin{itemize}
        \item Approvals
        \item Attention drawing
        \item Prohibition
        \item Comforting
        \item Neutral
    \end{itemize}

    \begin{center}
        \includegraphics[width=0.8\linewidth]{kismet-audio-processing}
    \end{center}

\end{frame}

\section{Speech recognition}

\begin{frame}{Speech recognition}

    See Guido's lectures, but here's a small heads up.

    Automated Speech Recognition has improved tremendously over the past few
    years.

    \begin{itemize}
        \item Improved recognition performance
        \item Speaker independence
        \item Can deal with a larger amount of background noise
    \end{itemize}

    Plenty of reports on increased performance

\begin{itemize}

\item
  ``\href{https://blogs.microsoft.com/next/2016/10/18/historic-achievement-microsoft-researchers-reach-human-parity-conversational-speech-recognition/\#g5qKXHrZZ2pbxuPH.99}{Historic
  Achievement: Microsoft researchers reach human parity in
  conversational speech recognition}'' (Oct 2016)
\item
  Error rate (\% of misunderstood words) on
  \href{https://catalog.ldc.upenn.edu/LDC2004S13}{Switchboard} (SWB) and
  CallHome (CH) corpus.
\end{itemize}

    \begin{center}
        \includegraphics[width=0.8\linewidth]{asr-error-rates}
    \end{center}

\end{frame}

\begin{frame}{Speech recognition}

So what has changed?

\begin{itemize}

\item Introduction of new models for speech recognition based on Deep Neural
  Networks (DNN) or Convolutional Neural Networks (CNN), replacing
  models based on n-grams or Hidden Markov Models (HMM).

        \begin{center}
            \includegraphics[width=0.8\linewidth]{asr-evolution}
        \end{center}
\item More availability of training data, often collected through speech
  command interfaces (such as Google Now, Microsoft's Xbox chat
  function).
\item Computational power to train DNN has increased: High Performance
  Computing clusters and Graphical Processing Units (GPUs).
\item Recognition happens in the cloud, rather than on-board a device. This
  offers more computational power and access to large networks.
\end{itemize}

\end{frame}

\imageframe[caption=Open-source efforts by Mozilla]{common-voice-project}

\begin{frame}{Speech recognition}

Question

\begin{itemize}

\item
  So, given that ASR performance seems high, how good is it really for
  Human-Robot Interaction?
\end{itemize}

Human-Robot Interaction is unique

\begin{itemize}

\item
  There is often a noise environment.
\item
  Users are at a distance from the robot (and any speakers carried on
  the robot). Almost all ASR engines have been trained on speech
  recorded close to the microphone (\textless{} 10cm).
\item
  There might be a multi-party conversation (i.e.~multiple speakers).
\end{itemize}

\end{frame}

\begin{frame}{Speech recognition for children}

How well does speech recognition perform for child speech?

\begin{itemize}

\item
  For counting (1,2,3, \ldots{}) and short sentences \textbf{spoken by
  adults}, recognition rate is \textbf{90\% using Nao microphone and
  on-board ASR} (Nuance VoCon 4.7). And up to 99\% recognition with a
  high-quality microphone and Google ASR.
\item
  Can we assume that ASR on children's speech will have the same
  performance?
\end{itemize}

Child speech is very different from adult speech.

\begin{itemize}

\item
  Higher pitch (due to small vocal tracts).
\item
  higher number of disfluencies and, especially in younger children,
  language utterances are often ungrammatical (e.g. ``The boy
  \emph{putted} the frog in the box'').
\end{itemize}

\end{frame}

\imageframe{child-speech}

\begin{frame}{Examples of recordings (cleaned-up, normalised)}

\begin{itemize}

\item
  Numbers (0 to 10)
\item
  Sentences
\item
  Spontaneous speech
\end{itemize}

Jacob, native English speaker

Julia, native English speaker

Clean

Noisy

(Recorded with studio microphone)

\end{frame}

\begin{frame}{Impact of microphone type}

    \begin{center}
        \includegraphics[width=0.8\linewidth]{mic_graph}
    \end{center}

Recognition rate of numbers spoken by children, split by microphone type
(62 utterances) using Nao's ASR engine (Nuance Vocon 4.7) constrained by
a grammar containing only number words.

\end{frame}

\begin{frame}{Impact of background noise}

\begin{center}
    \includegraphics[width=0.8\linewidth]{noise_graph}
\end{center}

Recognition rate of number utterances spoken by children, split by
background noise level (83 total utterances) using Nao's ASR engine
(Nuance Vocon 4.7) constrained by a grammar containing only number
words.

Clean

Background noise

\end{frame}

\begin{frame}{Location of speaker wrt. Nao}

\only<1>{
    \begin{columns}
        \begin{column}{0.5\linewidth}
            \begin{center}
                \includegraphics[width=0.8\linewidth]{recording-audio-nao}
            \end{center}
        \end{column}
        \begin{column}{0.5\linewidth}
    \begin{center}
        \includegraphics[width=0.8\linewidth]{directional_recordings}
    \end{center}
        \end{column}
    \end{columns}

}

    \only<2>{
        \begin{center}
            \includegraphics[width=0.8\linewidth]{heatmaps}
        \end{center}
    }


\end{frame}

\begin{frame}{Classification}

Social signal processing often relies on \textbf{classification.}

\begin{itemize}

\item
  \href{https://en.wikipedia.org/wiki/Statistical_classification}{\textbf{Classification}}
  is deciding on which \textbf{category} a new observation belongs to
  based on training data.
\item
  The training data contains data and known categories.
\item
  Classification is a \textbf{supervised} learning algorithm.
\end{itemize}

Examples

\begin{itemize}

\item
  An incoming tweet needs to classified as being positive or negative.
\item
  An radar ping of a flying objects needs to be classified as belonging
  to one of \emph{n} possible planes.
\item
  The prosody of speech needs to be classified as belonging to one of
  six basic categories of emotion (happy, sad, angry, bored, surprised,
  neutral).
\item
  A gesture filmed through a camera needs to be classified as meaning
  stop, go, left or right.
\end{itemize}

\end{frame}

\begin{frame}{Classification: example}

\begin{itemize}

\item
  Two dimensional problem.
\item
  Two categories, with training data for both categories.
\item
  To which category does a new observation belong?
\end{itemize}

?

\end{frame}

\begin{frame}{Classification}

\begin{itemize}

\item
  When categories can be linearly separated, we have a very easy
  classification problem
\end{itemize}

\end{frame}

\begin{frame}{Classification}

\begin{itemize}

\item
  Three or more can still be linearly separated\ldots{}
\end{itemize}

\end{frame}

\begin{frame}{Classification}

\begin{itemize}

\item
  But what if categories are not linearly separable?
\end{itemize}

\end{frame}

\begin{frame}{Two classification methods}

Two methods will be explained here

\begin{itemize}

\item
  \(k\) Nearest Neighbour (kNN)
\item
  Support Vector Machines (SVM)
\end{itemize}

But there are hundreds of classifiers

\begin{itemize}

\item
  Decision trees
\item
  Random forest
\item
  Bayes classifiers
\item
  Neural Networks
\item
  \ldots{}
\end{itemize}

\end{frame}

\begin{frame}{\(k\) Nearest Neighbour}

\begin{itemize}

\item
  When an observation comes in, calculate the distance to \(k\) nearest
  neighbours, the observation belongs to the most common class.
\end{itemize}

\end{frame}

\begin{frame}{\(k\) Nearest Neighbour}

Choosing \(k\)

\begin{itemize}

\item
  Choose too small and it sensitive to classification noise.
\item
  Choose too large and classification accuracy decreases.
\end{itemize}

Decision border with \(k\)=1 and \(k\)=2

\end{frame}

\begin{frame}[fragile]{\(k\) Nearest Neighbour}

\begin{block}{Python code for k Nearest Neighbour}

\begin{pythoncode}
def knn(k, data, dataClass, inputs):
  nInputs = np.shape(inputs)[0]
  closest = np.zeros(nInputs)

  for n in range(nInputs):
  
  # Compute distances
  distances = np.sum((data - inputs[n,:])\\2,axis=1)
  
  # Identify the nearest neighbours
  indices = np.argsort(distances,axis=0)
  classes = np.unique(dataClass[indices[:k]])
  
  if len(classes)==1:
    closest[n] = np.unique(classes)
  else:
    counts = np.zeros(max(classes)+1)
  
  for i in range(k):
    counts[dataClass[indices[i]]] += 1
  
  closest[n] = np.argmax(counts,axis=0)
  return closest
\end{pythoncode}



\(k\) Nearest Neighbour usually does really well on a large number of
classification problem, but can underperform near the classification
boundary.

Struggles with very large datasets, as it needs to calculate the
distance to all training data every time you classify a new observation.

\begin{itemize}

\item
  Some work around are available, relying on approximate distances and
  optimisation of the algorithmic code.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}{Support Vector Machines}

A very popular classification algorithm introduced by Vapnik in 1992.

Often (but not always) provides very impressive classification
performance on reasonably sized datasets.

SVMs do not work well on extremely large datasets, since (as we shall
see) the computations don't scale well with the number of training
examples, and so become computationally very expensive.

\begin{itemize}

\item
  Training a SVM requires \emph{O(m3)} operations and \emph{O(m2)}
  memory, with \emph{m} being the number of training samples\emph{.} For
  example, if you have 10,000 training samples, the training will take
  in the order of 1012 operations (imagine you can do 100,000
  operations/second, you will need 108 seconds  3 years to train your
  SVM and 1012 memory units.
\end{itemize}

As opposed to k Nearest Neighbour, SVM are more difficult to understand
(although no knowledge of the algorithms internals is needed to use it).

For more information on ``big O notation'', see time
\href{https://en.wikipedia.org/wiki/Time_complexity}{complexity of
algorithms}.

\end{frame}

\begin{frame}{SVM principle}

Three different classification lines. All are correct, but is there any
reason why one is better than the others?

Intuitively, the line that is most distant to the training data is the
best division.

\begin{itemize}

\item
  The empty area near the division line is symmetric. It forms a bar in
  2D space, a cylinder in 3D space, and a hyper-cylinder in n-D space.
\end{itemize}

\end{frame}

\begin{frame}{Classifier and Support vectors}

The classifier in the middle is called the \textbf{maximum margin
classifier}.

The data points nearest the classifier are called \textbf{support
vectors}.

Two observations

\begin{itemize}

\item
  The margins should be as large as possible.
\item
  The support vectors are the most useful datapoints because they are
  the ones that we might get wrong.
\end{itemize}

\end{frame}

\begin{frame}{Linear classifier}

A linear classifier for two categories can be written as
\(y = w \cdot x + b\)

\begin{itemize}

\item
  \textbf{w}~·~\textbf{x}~= ∑\emph{i}~\emph{wixi}. This is called the
  scalar product or inner product. Can also be written as a matrix
  multiplication~\textbf{w}T\textbf{x}
\item
  \textbf{w} is a weight vector, tilting the line
\item
  \textbf{x} is a data point
\item
  \emph{b} is a bias, lifting the line up along the y-axis
\end{itemize}

\textbf{w} is such that if for an \textbf{x} observation \emph{y} is
positive, then the observation belongs to one category (the `+'
category). **** If \emph{y} is negative, the observation belongs to the
`o' category.

In SVM we want a margin \emph{M}, so we can rewrite the linear
classifier. If \textbf{w}T\textbf{x}~+~\emph{b}~≥~\emph{M} than an
observation belongs to `+', for \textbf{w}T\textbf{x}~+~\emph{b}~≤
−\emph{M} it belongs to `o'.

Now suppose that we pick a point~\textbf{x}+~that lies on the `+' class
boundary line, so that~\textbf{w}T\textbf{x}+~=~\emph{M}. This is a
\textbf{support vector}.

\end{frame}

\begin{frame}{Finding the optimal classifier}

The problem is to find the \textbf{optimal} values for \textbf{w} and
\emph{b.}

The classifier needs to satisfy two conditions

\begin{itemize}

\item
  It needs to correctly classify the training data,
\item
  and needs the margin from the classifier to be as large as possible.
\end{itemize}

Let's assume the target values \emph{ti} of the categories are +1 and -1
(not 0 and 1), we can rewrite the equation for the linear classifier
as~\emph{ti}(\textbf{w}T\textbf{x}~+~\emph{b}) ≥ 1.

A \textbf{quadratic programming solver} is used to find the optimal
values for \textbf{w} and \emph{b.}

\end{frame}

\begin{frame}{Transformation of data}

\begin{itemize}

\item
  Unfortunately, all this still assumes that the data is linearly
  separable. But what if it isn't, and we are not prepared for a few
  misclassifications?
\item
  The solution is to \textbf{transform} the data: move data points in
  the n-D space until the training data is linearly separable again
\end{itemize}

\end{frame}

\begin{frame}{Kernel methods}

We can transform data points (= move them) or even add more dimensions.

We are using some function Θ(x\emph{i}) from input~x\emph{i}.

If we knew something about the data, then we might be able to identify
functions that would be a good idea.

\begin{itemize}

\item
  Example, separating odd from even numbers
\end{itemize}

But we often don't have domain knowledge.

A possible solution is to take \textbf{polynomials} of the input data:
for example everything up to degree 2

\begin{itemize}

\item
  Starting from input data \emph{x}1,~\emph{x}2, \ldots{},~\emph{xd,}
  square all data \emph{x}21,~\emph{x}22, \ldots{},~\emph{x}2\emph{d},
  and also take all products of the
  data~\emph{x}1\emph{x}2,~\emph{x}1\emph{x}3,
  \ldots{},~\emph{xd}−1\emph{xd}.~
\item
  We've transformed a data set of \emph{d} elements into a set with
  \emph{d}2/2~elements.
\end{itemize}



\begin{itemize}

\item
  For example, for one dimension \emph{x}1 the data is linearly
  inseparable. But adding a second dimension \emph{x}12 results in the
  data being separable.
\end{itemize}

\end{frame}

\begin{frame}{Which kernel to use?}

\begin{itemize}

\item
  As seen two slides ago, polynomials up to some degree~\emph{s}~in the
  elements~\emph{xk}~of the input vector
  (e.g.,~\emph{x}33~or~\emph{x}1\emph{x}4). This can be written as
\item
  Sigmoid functions of the~\emph{xk}s with parameters κ and δ, and
  kernel:
\item
  Radial basis function expansions of the~\emph{xk}s with parameter σ
  and kernel:
\end{itemize}



\begin{itemize}

\item
  Choosing which kernel to use and the parameters in these kernels is a
  tricky problem. While there is some theory based on something known as
  the Vapnik--Chernik dimension that can be applied, most people just
  experiment with different values and find one that works.
\item
  Let's experiment\ldots{} Install Weka
  \url{http://www.cs.waikato.ac.nz/ml/weka/}
\end{itemize}

\end{frame}

\begin{frame}{The iris data set}

Classifies three species of irises (flowers) on the bases of four
properties

\begin{itemize}

\item
  Sepal length, sepal width, petal length, petal width
\end{itemize}

\end{frame}

\begin{frame}{Confusion: clustering and classification}

\textbf{Clustering} is sometimes confused with \textbf{classification}.

\begin{itemize}

\item
  Often because some algorithms have similar names,e.g. \emph{k Means
  clustering} and \emph{k Nearest Neighbours}.
\end{itemize}

\textbf{Clustering} starts from unlabelled data points and tries to find
\(k\) clusters in the data.

\begin{itemize}

\item
  Used to find patterns in the data.
\end{itemize}

\textbf{Classification} starts from training data, and attempts to
correctly classify and unknown observation.

\end{frame}

\begin{frame}{How to classify social signals?}

Raw signals will in most cases require pre-processing to extract
features.

The raw social signal (audio or video) requires pre-processing to
extract between 10 and over a 1000 \textbf{features}.

\begin{itemize}

\item
  A raw signals contains too much data, and cannot be fed to the
  classifier immediately.
\item
  Pre-processing extracts feature data which is relevant for the
  information which we are after (pitch, volume/energy, duration,
  formant frequencies, \ldots{})
\item
  These features then form the input for the classifier.
\end{itemize}

For more information see, for example, Liang et al. (2005)
\href{http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=1571239}{Feature
analysis and extraction for audio automatic classification}, Systems,
Man and Cybernetics, 2005 IEEE International Conference on.

Did you know\ldots{} An exception to this are Convolutional Neural
Networks, which can deal with unprocessed data.

\end{frame}

\begin{frame}{Example: recognising gender from speech}

Can we automatically recognise someone's gender from speech?

3,168 recorded voice samples, collected from male and female speakers.

\begin{itemize}

\item
  Examples from the database: male (US), female (US), male (Scotish)
\end{itemize}

The voice samples are pre-processed by acoustic analysis in R using the
seewave, warbleR and tuneR packages, with an analysed frequency range of
0hz-280hz (fundamental frequency of human speech).

\begin{itemize}

\item
  Extracted 20 features.
\end{itemize}

For data and more information see
\url{https://www.kaggle.com/primaryobjects/voicegender}

\end{frame}

\begin{frame}{Example: recognising gender from speech}

\begin{itemize}

\item
  \textbf{meanfreq}: mean frequency (in kHz)
\item
  \textbf{sd}: standard deviation of frequency
\item
  \textbf{median}: median frequency (in kHz)
\item
  \textbf{Q25}: first quantile (in kHz)
\item
  \textbf{Q75}: third quantile (in kHz)
\item
  \textbf{IQR}: interquantile range (in kHz)
\item
  \textbf{skew}: skewness (see note in specprop description)
\item
  \textbf{kurt}: kurtosis (see note in specprop description)
\item
  \textbf{sp.ent}: spectral entropy
\item
  \textbf{sfm}: spectral flatness
\item
  \textbf{mode}: mode frequency
\item
  \textbf{centroid}: frequency centroid (see specprop)
\item
  \textbf{peakf}: peak frequency (frequency with highest energy)
\end{itemize}

\begin{itemize}

\item
  \textbf{meanfun}: average of fundamental frequency measured across
  acoustic signal
\item
  \textbf{minfun}: minimum fundamental frequency measured across
  acoustic signal
\item
  \textbf{maxfun}: maximum fundamental frequency measured across
  acoustic signal
\item
  \textbf{meandom}: average of dominant frequency measured across
  acoustic signal
\item
  \textbf{mindom}: minimum of dominant frequency measured across
  acoustic signal
\item
  \textbf{maxdom}: maximum of dominant frequency measured across
  acoustic signal
\item
  \textbf{dfrange}: range of dominant frequency measured across acoustic
  signal
\item
  \textbf{modindx}: modulation index. Calculated as the accumulated
  absolute difference between adjacent measurements of fundamental
  frequencies divided by the frequency range
\end{itemize}

The 20 features used for gender classification

\end{frame}

\begin{frame}{Example: recognising gender from speech}

\begin{block}{Some examples of features vectors}

\end{block}

\end{frame}

\begin{frame}{Example: recognising gender from speech}

Performance

\begin{itemize}

\item
  kNN (k = 7): 97.8\% classified correctly.
\item
  SVM: 97.5\% classified correctly.
\end{itemize}

Recognising gender from speech is easy and robust.

\begin{itemize}

\item
  All classification algorithms can deal with this problem (at least all
  the ones I tried).
\end{itemize}

\end{frame}

\begin{frame}{Conclusion}

Social signal processing is extracting relevant information from the
social environment.

Some work relatively well

\begin{itemize}

\item
  Face \textbf{detection}, voice activity detection, gender
  classification, \ldots{}
\end{itemize}

Some work, but need improvement

\begin{itemize}

\item
  Gaze detection, basic emotion recognition, face \textbf{recognition},
  speech recognition, \ldots{}
\end{itemize}

But still many open problems remaining

\begin{itemize}

\item
  Complex real-word affect and emotion recognition (e.g.~embarrassment,
  pride).
\item
  Speech recognition for atypical speakers (children, elderly),
  multi-party interaction, \ldots{}
\end{itemize}

\end{frame}


\begin{frame}{}
    \begin{center}
        \Large
        That's all, folks!\\[2em]
        \normalsize
        Questions:\\
        Portland Square A216 or \url{severin.lemaignan@plymouth.ac.uk} \\[1em]

        Slides:\\ \href{https://github.com/severin-lemaignan/module-mobile-and-humanoid-robots}{\small github.com/severin-lemaignan/module-mobile-and-humanoid-robots}

    \end{center}
\end{frame}



\end{document}
